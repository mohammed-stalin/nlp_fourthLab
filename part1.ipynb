{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A-zsqhUmzoqw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8LhS4Mlzoqz"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"./data/hess_article_content.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtlFolhdzoq3"
      },
      "outputs": [],
      "source": [
        "df=df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsZCOwbZzoq4",
        "outputId": "74277dd1-a349-4a30-ee45-8f961fedb42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
          ]
        }
      ],
      "source": [
        "import string\n",
        "for c in string.punctuation:\n",
        "    print(c, end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLb4QpOxzoq5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJBRDtA8zoq6",
        "outputId": "4ecced07-fc0c-40cf-a587-194d28b09aec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEcGG5GYzoq7",
        "outputId": "327d0c5c-73ec-44f0-a1a6-a4b9532860bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwg0fHrizoq7",
        "outputId": "5c8150e0-b1d7-449d-bd39-a134e49c0c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop words:  {'أنتن', 'بعد', 'كأين', 'هَذَيْنِ', 'ذوا', 'تشرين', 'إلا', 'ف', 'شتان', 'ثلاث', 'ما أفعله', 'غير', 'هن', 'عشرين', 'حتى', 'تسعمائة', 'بطآن', 'سمعا', 'حرى', 'عين', 'كأنما', 'أخٌ', 'لهن', 'كأيّن', 'لسن', 'هم', 'كليكما', 'مليم', 'فضلا', 'مكانكنّ', 'كلَّا', 'عيانا', 'مئتان', 'ليرة', 'فيها', 'ذواتا', 'حادي', 'لكي', 'أو', 'خميس', 'تِي', 'أفعل به', 'باء', 'سوى', 'صبرا', 'قاطبة', 'تينك', 'عليه', 'أحد', 'هلا', 'سبعمائة', 'يمين', 'بات', 'ثمانين', 'ليسا', 'ميم', 'سبعين', 'منها', 'لاسيما', 'تعلَّم', 'نيف', 'حجا', 'درهم', 'أف', 'ذهب', 'أخو', 'أفٍّ', 'هاتان', 'لست', 'وإذ', 'إذا', 'صهْ', 'أمسى', 'أما', 'سبعة', 'إنما', 'سادس', 'رابع', 'تلكم', 'ي', 'أنّى', 'ليست', 'أيضا', 'تِه', 'إن', 'ت', 'فلا', 'أيلول', 'ألف', 'دال', 'ثمانية', 'عدا', 'أي', 'لستم', 'ستة', 'تسعون', 'لكنما', 'أربعاء', 'أينما', 'م', 'رأى', 'سبعمئة', 'ب', 'تي', 'اللذان', 'لعلَّ', 'إياكم', 'لمّا', 'بكم', 'حدَث', 'زعم', 'أنتما', 'إلّا', 'هكذا', 'ذِه', 'بما', 'سين', 'تين', 'إذ', 'آمينَ', 'الذين', 'وُشْكَانَ', 'تارة', 'اللائي', 'أجل', 'لبيك', 'حبيب', 'ض', 'ما انفك', 'جميع', 'بك', 'أنى', 'أغسطس', 'كيت', 'فو', 'حَذارِ', 'كان', 'ؤ', 'مما', 'إيانا', 'هَاتانِ', 'خال', 'خمسمئة', 'حسب', 'سبعون', 'قبل', 'ذِي', 'بهن', 'إياهم', 'ومن', 'مرّة', 'تحوّل', 'آذار', 'به', 'أنشأ', 'ح', 'لعمر', 'علق', 'حيثما', 'لئن', 'إياها', 'بخ', 'أوّهْ', 'أبريل', 'ين', 'اثنا', 'أنتِ', 'كم', 'بي', 'لكنَّ', 'أُفٍّ', 'أمامك', 'لا سيما', 'ثلاثمئة', 'لدن', 'أكتوبر', 'ضحوة', 'أعلم', 'ة', 'يوليو', 'أخبر', 'مثل', 'إياك', 'مارس', 'بئس', 'لكم', 'شين', 'مئة', 'تموز', 'مه', 'ذات', 'واحد', 'تخذ', 'غدا', 'شيكل', 'أبٌ', 'لما', 'ذا', 'بهما', 'ظاء', 'اتخذ', 'عشرون', 'قاف', 'هلم', 'سقى', 'إيهٍ', 'أرى', 'أكثر', 'ما', 'عل', 'خمس', 'ريث', 'ليستا', 'هللة', 'أضحى', 'أى', 'تجاه', 'آنفا', 'إنا', 'هَاتِه', 'ءَ', 'حزيران', 'بؤسا', 'ق', 'ك', 'ثمّ', 'كثيرا', 'تفعلون', 'أين', 'ستمائة', 'في', 'وإذا', 'هذي', 'ستون', 'لستما', 'كِخ', 'أبو', 'سنتيم', 'هَجْ', 'ثمانون', 'بسّ', 'كأي', 'بعدا', 'ولو', 'تسعمئة', 'أل', 'ذواتي', 'ذين', 'والذي', 'فلس', 'بعض', 'بيد', 'ذلكم', 'أول', 'إياهن', 'مافتئ', 'أمد', 'فيم', 'صاد', 'كذلك', 'كاد', 'رجع', 'عما', 'خلف', 'نيسان', 'طاق', 'لك', 'هذا', 'عاشر', 'ل', 'لستن', 'بَسْ', 'رويدك', 'كليهما', 'على', 'نون', 'حيَّ', 'إذاً', 'ج', 'د', 'والذين', 'ذاك', 'حاء', 'وراءَك', 'كلّما', 'ئ', 'أولالك', 'ابتدأ', 'بكن', 'إياي', 'تفعلين', 'واهاً', 'لكيلا', 'يورو', 'راح', 'أربعة', 'أسكن', 'عن', 'لدى', 'علم', 'جلل', 'جنيه', 'درى', 'أصبح', 'وا', 'كما', 'تسع', 'زود', 'ثمّة', 'قد', 'لا', 'أفريل', 'اللتيا', 'زاي', 'ست', 'هَذِه', 'لهم', 'هنالك', 'هاء', 'اخلولق', 'عند', 'عشرة', 'ذيت', 'بخٍ', 'معاذ', 'أنت', 'أوت', 'هاك', 'ستمئة', 'صباح', 'هذين', 'فوق', 'إليك', 'نا', 'فإن', 'لم', 'بغتة', 'جيم', 'ص', 'ثمنمئة', 'مادام', 'ولا', 'سابع', 'مكانَك', 'هَؤلاء', 'ا', 'سرا', 'تسعة', 'بَلْهَ', 'لوما', 'فيفري', 'ياء', 'هاتين', 'ذلك', 'أيار', 'أمامكَ', 'ماي', 'أولئك', 'وجد', 'أعطى', 'تحت', 'قطّ', 'هل', 'ثالث', 'طاء', 'فاء', 'ن', 'هَذِي', 'إليكم', 'أولاء', 'آض', 'حاشا', 'تاسع', 'يونيو', 'أربع', 'فيه', 'كانون', 'بين', 'تلقاء', 'غ', 'إي', 'إزاء', 'بضع', 'عليك', 'أن', 'بماذا', 'كيف', 'ارتدّ', 'ه', 'هيهات', 'كسا', 'لو', 'ثمة', 'تفعلان', 'ذَيْنِ', 'آه', 'ساء', 'أقبل', 'وَيْ', 'عشر', 'خلافا', 'مساء', 'إنه', 'ألا', 'أوه', 'أجمع', 'نفس', 'استحال', 'خلا', 'بهم', 'آهاً', 'سبحان', 'غين', 'ثان', 'لكما', 'واو', 'فلان', 'مع', 'لنا', 'اربعين', 'كن', 'غداة', 'حمدا', 'آب', 'عَدَسْ', 'آي', 'تلكما', 'دينار', 'رُبَّ', 'جويلية', 'ثمَّ', 'ذه', 'إليكما', 'ء', 'كرب', 'لسنا', 'لها', 'ذ', 'فمن', 'خ', 'علًّ', 'يفعلون', 'لولا', 'الآن', 'ذانِ', 'همزة', 'إياه', 'سحقا', 'ثمان', 'ثلاثين', 'طالما', 'سبت', 'حمٌ', 'كل', 'أيّ', 'أنتم', 'إياكن', 'سوف', 'آهٍ', 'ثاني', 'ظ', 'عاد', 'خبَّر', 'ضاد', 'اربعون', 'هيا', 'نعم', 'هنا', 'تسعين', 'يفعلان', 'خمسين', 'اثنان', 'جير', 'أربعمائة', 'ذي', 'إذن', 'ثلاثمائة', 'ذانك', 'رزق', 'اللتين', 'لكن', 'فرادى', 'سبع', 'هؤلاء', 'أطعم', 'التي', 'مهما', 'لعل', 'هاكَ', 'بمن', 'أم', 'نحو', 'ث', 'ط', 'هذان', 'إلَيْكَ', 'وهب', 'عدَّ', 'ليت', 'طَق', 'خامس', 'أهلا', 'شمال', 'جمعة', 'ذينك', 'هاهنا', 'ثم', 'اثني', 'ليسوا', 'ذو', 'أيا', 'هَيْهات', 'كى', 'بس', 'الذي', 'بل', 'دواليك', 'مائة', 'ممن', 'جعل', 'من', 'تانِ', 'شبه', 'إياهما', 'ذلكن', 'هما', 'حبذا', 'راء', 'نَخْ', 'آ', 'حاي', 'تعسا', 'لات', 'بنا', 'أيها', 'ها', 'أمّا', 'ش', 'اللذين', 'ثماني', 'صراحة', 'قام', 'هَذا', 'كأن', 'تلك', 'نحن', 'هي', 'هيت', 'ثامن', 'هلّا', 'قلما', 'هاته', 'أبدا', 'وهو', 'فبراير', 'إليكنّ', 'خاصة', 'حمو', 'لهما', 'إحدى', 'ثلاثون', 'مذ', 'ع', 'ستين', 'يوان', 'تانِك', 'الألى', 'أ', 'جانفي', 'صبر', 'فيما', 'تاء', 'ته', 'اللتان', 'مكانكما', 'صدقا', 'س', 'إليكَ', 'خاء', 'انبرى', 'نبَّا', 'ترك', 'ألفى', 'ّأيّان', 'شَتَّانَ', 'دون', 'له', 'عوض', 'إذما', 'عامة', 'إنَّ', 'ورد', 'هناك', 'يا', 'شتانَ', 'ليس', 'كي', 'تبدّل', 'هبّ', 'و', 'أقل', 'أيّان', 'انقلب', 'كلما', 'آها', 'لام', 'هاتي', 'تَيْنِ', 'غالبا', 'بها', 'حار', 'إليكن', 'سرعان', 'فإذا', 'طرا', 'آهِ', 'ماذا', 'مازال', 'أمام', 'اللاتي', 'مايو', 'هَذانِ', 'أربعمئة', 'ديسمبر', 'إيه', 'حين', 'ظلّ', 'ثلاثة', 'كلا', 'قرش', 'ما برح', 'اللواتي', 'ى', 'لي', 'أصلا', 'هذه', 'ظنَّ', 'ذال', 'عسى', 'مكانكم', 'شباط', 'منه', 'إياكما', 'إمّا', 'كذا', 'أمس', 'ذلكما', 'أنا', 'اثنين', 'وإن', 'طفق', 'كلاهما', 'جوان', 'شرع', 'ز', 'بكما', 'سبتمبر', 'ثلاثاء', 'ولكن', 'كاف', 'نَّ', 'أخذ', 'لن', 'عجبا', 'أنبأ', 'كأيّ', 'حقا', 'خمسون', 'غادر', 'ر', 'هيّا', 'هَاتِي', 'إلى', 'ثاء', 'كلتا', 'خمسمائة', 'هَاتَيْنِ', 'كيفما', 'ريال', 'إى', 'صهٍ', 'كأنّ', 'هو', 'ذان', 'وما', 'متى', 'خمسة', 'الألاء', 'أوشك', 'دونك', 'يناير', 'دولار', 'إما', 'منذ', 'ثمانمئة', 'آناء', 'بلى', 'صار', 'أنًّ', 'نوفمبر', 'حيث'}\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('arabic'))\n",
        "print(\"Stop words: \",stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umykY47lzoq8"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s,]', '', text)\n",
        "    # Supprimer les caractères qui ne sont pas en arabe\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('arabic'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn04RAU5zoq8",
        "outputId": "8b24fb9e-d380-4ca9-d5df-115c1006d284"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>score</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ذات يوم قال ستالين، وهو بالمناسبة أبشع أمين عا...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>يوم قال ستالين بالمناسبة أبشع أمين عام حزب منت...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>قال الرئيس التركي رجب طيب أردوغان إن بنيامين ن...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>قال الرئيس التركي رجب طيب أردوغان بنيامين نتني...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>دخل حزب الأصالة والمعاصرة في الآونة الأخيرة، م...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>دخل حزب الأصالة والمعاصرة الآونة الأخيرة مرحلة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>استطاع الفلسطينيون أن يحققوا انتصارا كبيرا على...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>استطاع الفلسطينيون يحققوا انتصارا كبيرا إسرائي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>لا يزال جثمان لينين المحنط مسجى في ضريحه في ال...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>يزال جثمان لينين المحنط مسجى ضريحه الساحة الحم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>شرعية(بمعنى الحفاظ على استقرار الوطن المغربي) ...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>شرعيةبمعنى الحفاظ استقرار الوطن المغربي الملك ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>أعلن مؤخرا الحبيب المالكي (الصورة) عضو المكتب ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>أعلن مؤخرا الحبيب المالكي الصورة عضو المكتب ال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>كان الكثير من المغاربة يُقدرون أن الحسن الثان...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>الكثير المغاربة يقدرون الحسن الثاني قابل للموت...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>أربعون سنة مرت على ثورة الشباب في اروبة في ماي...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>أربعون سنة مرت ثورة الشباب اروبة أربعون سنة مر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>مرت أمس الأربعاء الذكرى الخامسة والخمسون لوفاة...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>مرت الأربعاء الذكرى الخامسة والخمسون لوفاة الش...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  score  \\\n",
              "0    ذات يوم قال ستالين، وهو بالمناسبة أبشع أمين عا...   10.0   \n",
              "1    قال الرئيس التركي رجب طيب أردوغان إن بنيامين ن...    2.0   \n",
              "2    دخل حزب الأصالة والمعاصرة في الآونة الأخيرة، م...    4.0   \n",
              "3    استطاع الفلسطينيون أن يحققوا انتصارا كبيرا على...    5.0   \n",
              "4    لا يزال جثمان لينين المحنط مسجى في ضريحه في ال...   10.0   \n",
              "..                                                 ...    ...   \n",
              "220  شرعية(بمعنى الحفاظ على استقرار الوطن المغربي) ...    3.0   \n",
              "221  أعلن مؤخرا الحبيب المالكي (الصورة) عضو المكتب ...   10.0   \n",
              "222   كان الكثير من المغاربة يُقدرون أن الحسن الثان...    9.0   \n",
              "223  أربعون سنة مرت على ثورة الشباب في اروبة في ماي...   10.0   \n",
              "224  مرت أمس الأربعاء الذكرى الخامسة والخمسون لوفاة...   10.0   \n",
              "\n",
              "                                            clean_text  \n",
              "0    يوم قال ستالين بالمناسبة أبشع أمين عام حزب منت...  \n",
              "1    قال الرئيس التركي رجب طيب أردوغان بنيامين نتني...  \n",
              "2    دخل حزب الأصالة والمعاصرة الآونة الأخيرة مرحلة...  \n",
              "3    استطاع الفلسطينيون يحققوا انتصارا كبيرا إسرائي...  \n",
              "4    يزال جثمان لينين المحنط مسجى ضريحه الساحة الحم...  \n",
              "..                                                 ...  \n",
              "220  شرعيةبمعنى الحفاظ استقرار الوطن المغربي الملك ...  \n",
              "221  أعلن مؤخرا الحبيب المالكي الصورة عضو المكتب ال...  \n",
              "222  الكثير المغاربة يقدرون الحسن الثاني قابل للموت...  \n",
              "223  أربعون سنة مرت ثورة الشباب اروبة أربعون سنة مر...  \n",
              "224  مرت الأربعاء الذكرى الخامسة والخمسون لوفاة الش...  \n",
              "\n",
              "[220 rows x 3 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[:, 'clean_text'] = df['text'].apply(clean_text)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOffDkNozoq9"
      },
      "outputs": [],
      "source": [
        "import pyarabic.araby as araby"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wH7ofgizoq-"
      },
      "outputs": [],
      "source": [
        "from sre_parse import Tokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define a function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    tokens = araby.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Apply tokenization to each text in the DataFrame\n",
        "df.loc[:, 'tokenized_text'] = df['clean_text'].apply(tokenize_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8rU1WuIzoq-",
        "outputId": "161d4af9-781a-4daf-e9d5-99035c2a393f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>score</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>tokenized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ذات يوم قال ستالين، وهو بالمناسبة أبشع أمين عا...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>يوم قال ستالين بالمناسبة أبشع أمين عام حزب منت...</td>\n",
              "      <td>[يوم, قال, ستالين, بالمناسبة, أبشع, أمين, عام,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>قال الرئيس التركي رجب طيب أردوغان إن بنيامين ن...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>قال الرئيس التركي رجب طيب أردوغان بنيامين نتني...</td>\n",
              "      <td>[قال, الرئيس, التركي, رجب, طيب, أردوغان, بنيام...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>دخل حزب الأصالة والمعاصرة في الآونة الأخيرة، م...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>دخل حزب الأصالة والمعاصرة الآونة الأخيرة مرحلة...</td>\n",
              "      <td>[دخل, حزب, الأصالة, والمعاصرة, الآونة, الأخيرة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>استطاع الفلسطينيون أن يحققوا انتصارا كبيرا على...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>استطاع الفلسطينيون يحققوا انتصارا كبيرا إسرائي...</td>\n",
              "      <td>[استطاع, الفلسطينيون, يحققوا, انتصارا, كبيرا, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>لا يزال جثمان لينين المحنط مسجى في ضريحه في ال...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>يزال جثمان لينين المحنط مسجى ضريحه الساحة الحم...</td>\n",
              "      <td>[يزال, جثمان, لينين, المحنط, مسجى, ضريحه, السا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>شرعية(بمعنى الحفاظ على استقرار الوطن المغربي) ...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>شرعيةبمعنى الحفاظ استقرار الوطن المغربي الملك ...</td>\n",
              "      <td>[شرعيةبمعنى, الحفاظ, استقرار, الوطن, المغربي, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>أعلن مؤخرا الحبيب المالكي (الصورة) عضو المكتب ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>أعلن مؤخرا الحبيب المالكي الصورة عضو المكتب ال...</td>\n",
              "      <td>[أعلن, مؤخرا, الحبيب, المالكي, الصورة, عضو, ال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>كان الكثير من المغاربة يُقدرون أن الحسن الثان...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>الكثير المغاربة يقدرون الحسن الثاني قابل للموت...</td>\n",
              "      <td>[الكثير, المغاربة, يقدرون, الحسن, الثاني, قابل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>أربعون سنة مرت على ثورة الشباب في اروبة في ماي...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>أربعون سنة مرت ثورة الشباب اروبة أربعون سنة مر...</td>\n",
              "      <td>[أربعون, سنة, مرت, ثورة, الشباب, اروبة, أربعون...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>مرت أمس الأربعاء الذكرى الخامسة والخمسون لوفاة...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>مرت الأربعاء الذكرى الخامسة والخمسون لوفاة الش...</td>\n",
              "      <td>[مرت, الأربعاء, الذكرى, الخامسة, والخمسون, لوف...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  score  \\\n",
              "0    ذات يوم قال ستالين، وهو بالمناسبة أبشع أمين عا...   10.0   \n",
              "1    قال الرئيس التركي رجب طيب أردوغان إن بنيامين ن...    2.0   \n",
              "2    دخل حزب الأصالة والمعاصرة في الآونة الأخيرة، م...    4.0   \n",
              "3    استطاع الفلسطينيون أن يحققوا انتصارا كبيرا على...    5.0   \n",
              "4    لا يزال جثمان لينين المحنط مسجى في ضريحه في ال...   10.0   \n",
              "..                                                 ...    ...   \n",
              "220  شرعية(بمعنى الحفاظ على استقرار الوطن المغربي) ...    3.0   \n",
              "221  أعلن مؤخرا الحبيب المالكي (الصورة) عضو المكتب ...   10.0   \n",
              "222   كان الكثير من المغاربة يُقدرون أن الحسن الثان...    9.0   \n",
              "223  أربعون سنة مرت على ثورة الشباب في اروبة في ماي...   10.0   \n",
              "224  مرت أمس الأربعاء الذكرى الخامسة والخمسون لوفاة...   10.0   \n",
              "\n",
              "                                            clean_text  \\\n",
              "0    يوم قال ستالين بالمناسبة أبشع أمين عام حزب منت...   \n",
              "1    قال الرئيس التركي رجب طيب أردوغان بنيامين نتني...   \n",
              "2    دخل حزب الأصالة والمعاصرة الآونة الأخيرة مرحلة...   \n",
              "3    استطاع الفلسطينيون يحققوا انتصارا كبيرا إسرائي...   \n",
              "4    يزال جثمان لينين المحنط مسجى ضريحه الساحة الحم...   \n",
              "..                                                 ...   \n",
              "220  شرعيةبمعنى الحفاظ استقرار الوطن المغربي الملك ...   \n",
              "221  أعلن مؤخرا الحبيب المالكي الصورة عضو المكتب ال...   \n",
              "222  الكثير المغاربة يقدرون الحسن الثاني قابل للموت...   \n",
              "223  أربعون سنة مرت ثورة الشباب اروبة أربعون سنة مر...   \n",
              "224  مرت الأربعاء الذكرى الخامسة والخمسون لوفاة الش...   \n",
              "\n",
              "                                        tokenized_text  \n",
              "0    [يوم, قال, ستالين, بالمناسبة, أبشع, أمين, عام,...  \n",
              "1    [قال, الرئيس, التركي, رجب, طيب, أردوغان, بنيام...  \n",
              "2    [دخل, حزب, الأصالة, والمعاصرة, الآونة, الأخيرة...  \n",
              "3    [استطاع, الفلسطينيون, يحققوا, انتصارا, كبيرا, ...  \n",
              "4    [يزال, جثمان, لينين, المحنط, مسجى, ضريحه, السا...  \n",
              "..                                                 ...  \n",
              "220  [شرعيةبمعنى, الحفاظ, استقرار, الوطن, المغربي, ...  \n",
              "221  [أعلن, مؤخرا, الحبيب, المالكي, الصورة, عضو, ال...  \n",
              "222  [الكثير, المغاربة, يقدرون, الحسن, الثاني, قابل...  \n",
              "223  [أربعون, سنة, مرت, ثورة, الشباب, اروبة, أربعون...  \n",
              "224  [مرت, الأربعاء, الذكرى, الخامسة, والخمسون, لوف...  \n",
              "\n",
              "[220 rows x 4 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pP2N2wCkzoq_"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"data_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r6qa7jszzoq_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g-_KMvcVzorA"
      },
      "outputs": [],
      "source": [
        "data=df\n",
        "# Preprocessing\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['clean_text'])\n",
        "X = tokenizer.texts_to_sequences(data['clean_text'])\n",
        "X = pad_sequences(X)\n",
        "\n",
        "# Normalize target variable\n",
        "scaler = StandardScaler()\n",
        "y=data[\"score\"]\n",
        "y = scaler.fit_transform(y.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D665RjitzorA"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aUBDQgWXzorA"
      },
      "outputs": [],
      "source": [
        "# Define model architectures\n",
        "def build_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        LSTM(64),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_bidirectional_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_gru_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        GRU(64),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        LSTM(64),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Train the models\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=64):\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UttGx263zorB",
        "outputId": "f9c47507-5ab8-4351-e4a4-ff7d6ff24d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 5s 523ms/step - loss: 1.0019 - val_loss: 1.0684\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 1s 455ms/step - loss: 0.9609 - val_loss: 1.0628\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 1s 434ms/step - loss: 0.9097 - val_loss: 1.0566\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 1s 400ms/step - loss: 0.8165 - val_loss: 1.0381\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 1s 286ms/step - loss: 0.6636 - val_loss: 0.9981\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 1s 291ms/step - loss: 0.3789 - val_loss: 0.8667\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 1s 283ms/step - loss: 0.2026 - val_loss: 0.9318\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 1s 289ms/step - loss: 0.0842 - val_loss: 1.0206\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 1s 288ms/step - loss: 0.0790 - val_loss: 1.0605\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 1s 292ms/step - loss: 0.0592 - val_loss: 1.0542\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 5s 730ms/step - loss: 1.0038 - val_loss: 1.0779\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 2s 630ms/step - loss: 0.9766 - val_loss: 1.0787\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 2s 614ms/step - loss: 0.9455 - val_loss: 1.0739\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 1s 482ms/step - loss: 0.8878 - val_loss: 1.0695\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 1s 409ms/step - loss: 0.7908 - val_loss: 1.0499\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 1s 417ms/step - loss: 0.6103 - val_loss: 1.0027\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 1s 410ms/step - loss: 0.3123 - val_loss: 0.7764\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 1s 414ms/step - loss: 0.1152 - val_loss: 1.0042\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 1s 409ms/step - loss: 0.0880 - val_loss: 1.0718\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 1s 405ms/step - loss: 0.0726 - val_loss: 1.0475\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 4s 732ms/step - loss: 1.0067 - val_loss: 1.0712\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 1s 425ms/step - loss: 0.9646 - val_loss: 1.0656\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 1s 292ms/step - loss: 0.9207 - val_loss: 1.0605\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 1s 287ms/step - loss: 0.8645 - val_loss: 1.0533\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 1s 288ms/step - loss: 0.7871 - val_loss: 1.0446\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 1s 286ms/step - loss: 0.6847 - val_loss: 1.0349\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 0.5467 - val_loss: 1.0215\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.3783 - val_loss: 1.0066\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.2056 - val_loss: 0.9866\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 1s 281ms/step - loss: 0.0955 - val_loss: 0.9903\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 3s 568ms/step - loss: 1.0042 - val_loss: 1.0763\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 1s 429ms/step - loss: 0.9686 - val_loss: 1.0764\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 1s 476ms/step - loss: 0.9230 - val_loss: 1.0754\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 1s 286ms/step - loss: 0.8490 - val_loss: 1.0698\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.7170 - val_loss: 1.0448\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 1s 289ms/step - loss: 0.4695 - val_loss: 0.9501\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 1s 293ms/step - loss: 0.4118 - val_loss: 0.9916\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 1s 288ms/step - loss: 0.1273 - val_loss: 1.0869\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 1s 285ms/step - loss: 0.1470 - val_loss: 1.1341\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 1s 292ms/step - loss: 0.1296 - val_loss: 1.1620\n",
            "2/2 [==============================] - 0s 81ms/step\n",
            "RNN Model MSE: 0.6944791383786655\n",
            "2/2 [==============================] - 1s 97ms/step\n",
            "Bidirectional RNN Model MSE: 0.7484024955467066\n",
            "2/2 [==============================] - 0s 48ms/step\n",
            "GRU Model MSE: 0.8553408533513654\n",
            "2/2 [==============================] - 0s 48ms/step\n",
            "LSTM Model MSE: 0.7714057610702812\n"
          ]
        }
      ],
      "source": [
        "# Build and train RNN model\n",
        "rnn_model = build_rnn_model(X_train.shape[1:])\n",
        "rnn_history = train_model(rnn_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train Bidirectional RNN model\n",
        "bidirectional_rnn_model = build_bidirectional_rnn_model(X_train.shape[1:])\n",
        "bidirectional_rnn_history = train_model(bidirectional_rnn_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train GRU model\n",
        "gru_model = build_gru_model(X_train.shape[1:])\n",
        "gru_history = train_model(gru_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = build_lstm_model(X_train.shape[1:])\n",
        "lstm_history = train_model(lstm_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Evaluate the models\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    return mse\n",
        "\n",
        "rnn_mse = evaluate_model(rnn_model, X_test, y_test)\n",
        "print(\"RNN Model MSE:\", rnn_mse)\n",
        "\n",
        "bidirectional_rnn_mse = evaluate_model(bidirectional_rnn_model, X_test, y_test)\n",
        "print(\"Bidirectional RNN Model MSE:\", bidirectional_rnn_mse)\n",
        "\n",
        "gru_mse = evaluate_model(gru_model, X_test, y_test)\n",
        "print(\"GRU Model MSE:\", gru_mse)\n",
        "\n",
        "lstm_mse = evaluate_model(lstm_model, X_test, y_test)\n",
        "print(\"LSTM Model MSE:\", lstm_mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding dropout w early stop 3la wed overfiting"
      ],
      "metadata": {
        "id": "-fbWZ4KGFh5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--H2A-HhzorC",
        "outputId": "38f18929-178e-4170-889e-68949b7b78b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 6s 789ms/step - loss: 1.0035 - val_loss: 1.0811\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 1s 418ms/step - loss: 0.9850 - val_loss: 1.0752\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 1s 418ms/step - loss: 0.9644 - val_loss: 1.0677\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 1s 445ms/step - loss: 0.9034 - val_loss: 1.0564\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 2s 585ms/step - loss: 0.7536 - val_loss: 1.0134\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 2s 599ms/step - loss: 0.3875 - val_loss: 0.8272\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 1s 416ms/step - loss: 0.2749 - val_loss: 0.9233\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 1s 418ms/step - loss: 0.1172 - val_loss: 0.9547\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 1s 425ms/step - loss: 0.1236 - val_loss: 0.9493\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 10s 1s/step - loss: 1.0025 - val_loss: 1.0671\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 2s 694ms/step - loss: 0.9909 - val_loss: 1.0646\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 2s 676ms/step - loss: 0.9911 - val_loss: 1.0614\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 2s 674ms/step - loss: 0.9453 - val_loss: 1.0630\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 2s 682ms/step - loss: 0.8667 - val_loss: 1.0547\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 3s 967ms/step - loss: 0.6788 - val_loss: 0.9920\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 2s 686ms/step - loss: 0.3291 - val_loss: 0.7872\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 2s 692ms/step - loss: 0.2728 - val_loss: 0.7719\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 2s 674ms/step - loss: 0.1214 - val_loss: 0.8828\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 2s 679ms/step - loss: 0.0840 - val_loss: 0.9290\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 5s 743ms/step - loss: 1.0048 - val_loss: 1.0762\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 1s 417ms/step - loss: 0.9861 - val_loss: 1.0741\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 1s 431ms/step - loss: 0.9666 - val_loss: 1.0713\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 2s 598ms/step - loss: 0.9457 - val_loss: 1.0724\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 2s 616ms/step - loss: 0.9043 - val_loss: 1.0707\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 1s 417ms/step - loss: 0.8322 - val_loss: 1.0677\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 1s 413ms/step - loss: 0.7024 - val_loss: 1.0626\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 1s 409ms/step - loss: 0.5082 - val_loss: 1.0526\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 1s 415ms/step - loss: 0.2346 - val_loss: 1.0503\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 1s 403ms/step - loss: 0.1959 - val_loss: 1.0741\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 7s 1s/step - loss: 0.9994 - val_loss: 1.0713\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 1s 448ms/step - loss: 0.9825 - val_loss: 1.0646\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 1s 421ms/step - loss: 0.9423 - val_loss: 1.0506\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 1s 422ms/step - loss: 0.8235 - val_loss: 1.0167\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 1s 431ms/step - loss: 0.5130 - val_loss: 0.8899\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 1s 417ms/step - loss: 0.2552 - val_loss: 0.9816\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 1s 435ms/step - loss: 0.1444 - val_loss: 1.1374\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 1s 426ms/step - loss: 0.1705 - val_loss: 1.0567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x79aa241ce170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 94ms/step\n",
            "RNN Model MSE: 0.9722462698586755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x79aa2c20ed40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 2s 249ms/step\n",
            "Bidirectional RNN Model MSE: 0.8611047630068002\n",
            "2/2 [==============================] - 1s 100ms/step\n",
            "GRU Model MSE: 0.7844308177772606\n",
            "2/2 [==============================] - 1s 91ms/step\n",
            "LSTM Model MSE: 0.919005333403069\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Define model architectures\n",
        "def build_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.5),\n",
        "        LSTM(64),\n",
        "        Dropout(0.5),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_bidirectional_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dropout(0.5),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_gru_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        GRU(64, return_sequences=True),\n",
        "        Dropout(0.5),\n",
        "        GRU(64),\n",
        "        Dropout(0.5),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.5),\n",
        "        LSTM(64),\n",
        "        Dropout(0.5),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Train the models\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=64):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
        "    return history\n",
        "\n",
        "# Build and train RNN model\n",
        "rnn_model = build_rnn_model(X_train.shape[1:])\n",
        "rnn_history = train_model(rnn_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train Bidirectional RNN model\n",
        "bidirectional_rnn_model = build_bidirectional_rnn_model(X_train.shape[1:])\n",
        "bidirectional_rnn_history = train_model(bidirectional_rnn_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train GRU model\n",
        "gru_model = build_gru_model(X_train.shape[1:])\n",
        "gru_history = train_model(gru_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = build_lstm_model(X_train.shape[1:])\n",
        "lstm_history = train_model(lstm_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Evaluate the models\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    return mse\n",
        "\n",
        "rnn_mse = evaluate_model(rnn_model, X_test, y_test)\n",
        "print(\"RNN Model MSE:\", rnn_mse)\n",
        "\n",
        "bidirectional_rnn_mse = evaluate_model(bidirectional_rnn_model, X_test, y_test)\n",
        "print(\"Bidirectional RNN Model MSE:\", bidirectional_rnn_mse)\n",
        "\n",
        "gru_mse = evaluate_model(gru_model, X_test, y_test)\n",
        "print(\"GRU Model MSE:\", gru_mse)\n",
        "\n",
        "lstm_mse = evaluate_model(lstm_model, X_test, y_test)\n",
        "print(\"LSTM Model MSE:\", lstm_mse)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Assuming your models are named as follows:\n",
        "rnn_model = rnn_model\n",
        "bidirectional_rnn_model =bidirectional_rnn_model\n",
        "gru_model = gru_model\n",
        "lstm_model = lstm_model\n",
        "\n",
        "# Save the GRU model using the native Keras format\n",
        "gru_model.save('gru2_model.h5')\n",
        "\n",
        "\n",
        "# Save the Bidirectional RNN model\n",
        "bidirectional_rnn_model.save('bidirectional_rnn_model.h5')\n",
        "\n",
        "# Save the GRU model\n",
        "gru_model.save('gru_model.h5')\n",
        "\n",
        "# Save the LSTM model\n",
        "lstm_model.save('lstm_model.h5')\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "gNaNrshu1GBH"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the GRU model using the native Keras format\n",
        "gru_model.save('gru_model_keras.keras')\n",
        "\n"
      ],
      "metadata": {
        "id": "O2f4V7kv2mbR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Load the GRU model\n",
        "gru2_model_keras = tf.keras.models.load_model('gru2_model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n"
      ],
      "metadata": {
        "id": "_mI97KdZ4YKR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarabic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWwvu4RNBLCb",
        "outputId": "628fe1e3-25a9-4d1d-e9b1-550618ced58c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarabic.araby as araby\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRCpGldgBIjh",
        "outputId": "41183aa9-7480-4c8d-83b5-9df2ba1fbe79"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your new Arabic text\n",
        "new_text = \"وُلد ستالين في مدينة غوري في الإمبراطورية الروسية لإسكافي يدعى فيساريو، وأم فلاحة تدعى «إيكاترينا». كانت عائلته تعيش في وضع اجتماعي يدعى القنانة وهو حالة من الرق أو العبودية. ستالين هو الولد الثالث للعائلة، لكن الولدين الأولين توفيا في مرحلة الطفولة نتيجةً للأمراض. أرادت أمه أن يصبح كاهنًا كعلامة شكر لله لأنه نجا من الموت خلافاً لإخوته. كان والد ستالين مدمنا على الكحول وكان دائم الضرب لستالين ولأمه؛ وفي أحد الأيام دفع الوالد ابنه أرضًا، ونتيجةً لهذه الضربة عانى ستالين من تصريف الدم مع البول لعدة أيام. استمر وضع الوالد بالتدهور حتى ترك عائلته ورحل وأصبحت أم ستالين بلا معيل، وذلك لأن النظام الاجتماعي في جورجيا هو نظام أبوي. وعندما بلغ ستالين 11 عامًا، أرسلته أمه إلى المدرسة الروسية للمسيحية الأرثوذكسية ودرس فيها. تعود بداية مشاركة ستالين مع الحركة الاشتراكية إلى فترة المدرسة الأرثوذكسية التي قامت بطرده من على مقاعد الدراسة في العام 1899 لعدم حضوره في الوقت المحدّد لتقديم الاختبارات. وبذلك خاب ظن أمه به حيث كانت تتمنى دائماً أن يكون كاهناً حتى بعد أن أصبح رئيساً. أصيب ستالين وهو في السابعة من عمره بمرض الجدري، وكانت على وجهه ندوب كثيرة بسبب المرض، لكنه تعافى منه. تعلم ستالين اللغة الروسية وهو في التاسعة من عمره لكنه ظل محتفظاً بلهجته الجورجية.\"\n",
        "new_text2=\"، وقد حدَّد ميثاق الأمم المتحدة الغاية من تأسيسها بالمحافظة على السلم والأمن الدوليين عن طريق اتخاذ تدابير جماعية فعَّالة لمنع وإزالة الأخطار التي تهدد السلام، وإلى تنمية العلاقات الودية بين الدول على أساس احترام مبدأ المساواة في الحقوق وتقرير المصير للشعوب وتعزيز وتشجيع احترام حقوق الإنسان والحريات الأساسية للجميع دون تمييز بسبب العرق أو الجنس أو اللغة أو الدين، بالإضافة لأن تكون مركزًا لتنسيق أعمال الدول في تحقيق هذه الغايات المشتركة. يقع المقر الرئيسي للأمم المتحدة على أرض دولية (حصانة محلية) في مدينة  \"\n",
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s,]', '', text)\n",
        "    # Supprimer les caractères qui ne sont pas en arabe\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('arabic'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "new_text = clean_text(new_text)\n",
        "new_text2 = clean_text(new_text2)\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "tokenizer1 = Tokenizer()\n",
        "tokenizer1.fit_on_texts([new_text])\n",
        "sequence1 = tokenizer1.texts_to_sequences([new_text])\n",
        "sequence1 = pad_sequences(sequence1)\n",
        "\n",
        "tokenizer2 = Tokenizer()\n",
        "tokenizer2.fit_on_texts([new_text2])\n",
        "sequence2 = tokenizer1.texts_to_sequences([new_text2])\n",
        "sequence2=pad_sequences(sequence2)"
      ],
      "metadata": {
        "id": "VFzqbLBL4gDG"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_score1 = gru_model.predict(sequence1)\n",
        "print(f\"Predicted Score: {predicted_score1[0][0]}\")\n",
        "\n",
        "predicted_score2 = gru_model.predict(sequence2)\n",
        "print(f\"Predicted Score: {predicted_score2[0][0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSVuLDcF49hs",
        "outputId": "4b6ac443-9812-420e-db01-332e347c098c"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Score: 0.21013334393501282\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted Score: -0.14173255860805511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Inverse transform the predicted score\n",
        "denormalized_score = scaler.inverse_transform(predicted_score1)[0][0]\n",
        "\n",
        "# Print the denormalized score\n",
        "print(f\"Denormalized Score 1: {denormalized_score}\")\n",
        "\n",
        "# Inverse transform the predicted score\n",
        "denormalized_score = scaler.inverse_transform(predicted_score2)[0][0]\n",
        "\n",
        "# Print the denormalized score\n",
        "print(f\"Denormalized Score 1: {denormalized_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DBjqBXd5BKj",
        "outputId": "e491a4db-8186-44f1-96bc-7f414dee3612"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Denormalized Score 1: 6.097904682159424\n",
            "Denormalized Score 1: 4.914048671722412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3if-V_Xy9sm2"
      },
      "execution_count": 111,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
